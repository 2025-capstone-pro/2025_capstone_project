{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f93fc1b",
   "metadata": {},
   "source": [
    "# MediaPipe로 랜드마크 추출\n",
    "\n",
    "- 총 12개의 랜드마크 추출.\n",
    "- 데이터셋은 모두 10초 영상으로 구성됨.\n",
    "- 이를 프레임 변환하면 모두 300 프레임으로 형성됨\n",
    "- 따라서 영상 전처리 시, 한 영상 당 (300, 12, 3) 의 벡터 형성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746543973.471201 1621901 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1746543973.489425 1641571 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA GeForce RTX 4090/PCIe/SSE2\n",
      "W0000 00:00:1746543973.565510 1641520 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1746543973.609534 1641535 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MediaPipe Pose 객체 생성 성공.\n",
      "결과 저장 디렉토리: ./../data/mediapipe_res/bad_innner_thigh\n",
      "\n",
      "총 235개의 비디오 파일을 처리합니다: ['0918_squat_000029.mp4', '0918_squat_000030.mp4', '0918_squat_000032.mp4', '0918_squat_000033.mp4', '0918_squat_000036.mp4', '0918_squat_000066.mp4', '0918_squat_000069.mp4', '0918_squat_000070.mp4', '0918_squat_000073.mp4', '0922_squat_000040.mp4', '0922_squat_000041.mp4', '0922_squat_000043.mp4', '0922_squat_000044.mp4', '0922_squat_000047.mp4', '0922_squat_000081.mp4', '0922_squat_000082.mp4', '0922_squat_000084.mp4', '0922_squat_000085.mp4', '0922_squat_000086.mp4', '0922_squat_000088.mp4', '0922_squat_000139.mp4', '0922_squat_000140.mp4', '0922_squat_000141.mp4', '0922_squat_000144.mp4', '0922_squat_000180.mp4', '0922_squat_000181.mp4', '0922_squat_000182.mp4', '0922_squat_000184.mp4', '0922_squat_000185.mp4', '0926_squat_000042.mp4', '0926_squat_000043.mp4', '0926_squat_000049.mp4', '0926_squat_000050.mp4', '0926_squat_000144.mp4', '0926_squat_000147.mp4', '0926_squat_000151.mp4', '0926_squat_000152.mp4', '1009_squat_000050.mp4', '1009_squat_000051.mp4', '1009_squat_000052.mp4', '1009_squat_000054.mp4', '1009_squat_000055.mp4', '1009_squat_000056.mp4', '1009_squat_000152.mp4', '1009_squat_000153.mp4', '1009_squat_000154.mp4', '1009_squat_000156.mp4', '1015_squat_000049.mp4', '1015_squat_000050.mp4', '1015_squat_000051.mp4', '1015_squat_000053.mp4', '1015_squat_000054.mp4', '1015_squat_000055.mp4', '1015_squat_000057.mp4', '1015_squat_000058.mp4', '1015_squat_000059.mp4', '1015_squat_000061.mp4', '1015_squat_000062.mp4', '1015_squat_000063.mp4', '1015_squat_000148.mp4', '1015_squat_000149.mp4', '1015_squat_000151.mp4', '1015_squat_000152.mp4', '1015_squat_000155.mp4', '1015_squat_000156.mp4', '1015_squat_000157.mp4', '1015_squat_000159.mp4', '1015_squat_000160.mp4', '1015_squat_000161.mp4', '1015_squat_000163.mp4', '1015_squat_000248.mp4', '1015_squat_000249.mp4', '1015_squat_000251.mp4', '1015_squat_000252.mp4', '1015_squat_000255.mp4', '1015_squat_000256.mp4', '1015_squat_000257.mp4', '1015_squat_000259.mp4', '1015_squat_000260.mp4', '1015_squat_000261.mp4', '1015_squat_000263.mp4', '1018_squat_000042.mp4', '1018_squat_000043.mp4', '1018_squat_000049.mp4', '1018_squat_000050.mp4', '1018_squat_000051.mp4', '1018_squat_000054.mp4', '1018_squat_000057.mp4', '1018_squat_000143.mp4', '1018_squat_000144.mp4', '1018_squat_000145.mp4', '1018_squat_000147.mp4', '1018_squat_000148.mp4', '1018_squat_000149.mp4', '1018_squat_000151.mp4', '1018_squat_000152.mp4', '1018_squat_000153.mp4', '1018_squat_000156.mp4', '1018_squat_000157.mp4', '1018_squat_000242.mp4', '1018_squat_000243.mp4', '1018_squat_000246.mp4', '1018_squat_000247.mp4', '1018_squat_000249.mp4', '1018_squat_000250.mp4', '1018_squat_000251.mp4', '1018_squat_000253.mp4', '1018_squat_000254.mp4', '1018_squat_000257.mp4', '1022_squat_000042.mp4', '1022_squat_000043.mp4', '1022_squat_000044.mp4', '1022_squat_000046.mp4', '1022_squat_000047.mp4', '1022_squat_000048.mp4', '1022_squat_000049.mp4', '1022_squat_000141.mp4', '1022_squat_000143.mp4', '1022_squat_000144.mp4', '1022_squat_000145.mp4', '1022_squat_000146.mp4', '1022_squat_000148.mp4', '1022_squat_000149.mp4', '1022_squat_000242.mp4', '1022_squat_000243.mp4', '1022_squat_000244.mp4', '1022_squat_000245.mp4', '1022_squat_000246.mp4', '1022_squat_000247.mp4', '1022_squat_000248.mp4', '1025_squat_000042.mp4', '1025_squat_000043.mp4', '1025_squat_000044.mp4', '1025_squat_000046.mp4', '1025_squat_000047.mp4', '1025_squat_000048.mp4', '1025_squat_000141.mp4', '1025_squat_000142.mp4', '1025_squat_000144.mp4', '1025_squat_000145.mp4', '1025_squat_000146.mp4', '1025_squat_000148.mp4', '1025_squat_000149.mp4', '1025_squat_000241.mp4', '1025_squat_000242.mp4', '1025_squat_000244.mp4', '1025_squat_000245.mp4', '1025_squat_000246.mp4', '1025_squat_000248.mp4', '1029_squat_000008.mp4', '1029_squat_000009.mp4', '1029_squat_000011.mp4', '1029_squat_000012.mp4', '1029_squat_000015.mp4', '1029_squat_000109.mp4', '1029_squat_000110.mp4', '1029_squat_000111.mp4', '1029_squat_000113.mp4', '1029_squat_000114.mp4', '1029_squat_000115.mp4', '1031_squat_000040.mp4', '1031_squat_000041.mp4', '1031_squat_000043.mp4', '1031_squat_000044.mp4', '1031_squat_000045.mp4', '1031_squat_000047.mp4', '1031_squat_000141.mp4', '1031_squat_000142.mp4', '1031_squat_000143.mp4', '1031_squat_000145.mp4', '1031_squat_000146.mp4', '1031_squat_000147.mp4', '1103_squat_000016.mp4', '1103_squat_000017.mp4', '1103_squat_000018.mp4', '1103_squat_000020.mp4', '1103_squat_000021.mp4', '1103_squat_000022.mp4', '1103_squat_000023.mp4', '1103_squat_000115.mp4', '1103_squat_000116.mp4', '1103_squat_000119.mp4', '1103_squat_000120.mp4', '1103_squat_000122.mp4', '1103_squat_000123.mp4', '1105_squat_000000.mp4', '1105_squat_000001.mp4', '1105_squat_000002.mp4', '1105_squat_000003.mp4', '1105_squat_000004.mp4', '1105_squat_000005.mp4', '1105_squat_000100.mp4', '1105_squat_000101.mp4', '1105_squat_000102.mp4', '1105_squat_000103.mp4', '1105_squat_000104.mp4', '1105_squat_000105.mp4', '1106_squat_000025.mp4', '1106_squat_000026.mp4', '1106_squat_000027.mp4', '1106_squat_000029.mp4', '1106_squat_000030.mp4', '1106_squat_000031.mp4', '1106_squat_000124.mp4', '1106_squat_000125.mp4', '1106_squat_000127.mp4', '1106_squat_000128.mp4', '1106_squat_000129.mp4', '1106_squat_000131.mp4', '1108_squat_000024.mp4', '1108_squat_000025.mp4', '1108_squat_000027.mp4', '1108_squat_000028.mp4', '1108_squat_000029.mp4', '1108_squat_000031.mp4', '1108_squat_000125.mp4', '1108_squat_000126.mp4', '1108_squat_000129.mp4', '1108_squat_000130.mp4', '1108_squat_000131.mp4', '1113_squat_000042.mp4', '1113_squat_000043.mp4', '1113_squat_000045.mp4', '1113_squat_000046.mp4', '1113_squat_000047.mp4', '1113_squat_000049.mp4', '1113_squat_000144.mp4', '1113_squat_000145.mp4', '1113_squat_000147.mp4', '1113_squat_000148.mp4', '1113_squat_000149.mp4', 'bad_squat_squat_000010.mp4', 'bad_squat_squat_000013.mp4', 'bad_squat_squat_000014.mp4', 'bad_squat_squat_000017.mp4']\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0918_squat_000029.mp4 ---\n",
      "'0918_squat_000029.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0918_squat_000029.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0918_squat_000029.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0918_squat_000029_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0918_squat_000030.mp4 ---\n",
      "'0918_squat_000030.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0918_squat_000030.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0918_squat_000030.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0918_squat_000030_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0918_squat_000032.mp4 ---\n",
      "'0918_squat_000032.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0918_squat_000032.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0918_squat_000032.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0918_squat_000032_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0918_squat_000033.mp4 ---\n",
      "'0918_squat_000033.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0918_squat_000033.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0918_squat_000033.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0918_squat_000033_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0918_squat_000036.mp4 ---\n",
      "'0918_squat_000036.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0918_squat_000036.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0918_squat_000036.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0918_squat_000036_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0918_squat_000066.mp4 ---\n",
      "'0918_squat_000066.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0918_squat_000066.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0918_squat_000066.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0918_squat_000066_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0918_squat_000069.mp4 ---\n",
      "'0918_squat_000069.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0918_squat_000069.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0918_squat_000069.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0918_squat_000069_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0918_squat_000070.mp4 ---\n",
      "'0918_squat_000070.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0918_squat_000070.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0918_squat_000070.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0918_squat_000070_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0918_squat_000073.mp4 ---\n",
      "'0918_squat_000073.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0918_squat_000073.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0918_squat_000073.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0918_squat_000073_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000040.mp4 ---\n",
      "'0922_squat_000040.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000040.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000040.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000040_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000041.mp4 ---\n",
      "'0922_squat_000041.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000041.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000041.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000041_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000043.mp4 ---\n",
      "'0922_squat_000043.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000043.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000043.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000043_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000044.mp4 ---\n",
      "'0922_squat_000044.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000044.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000044.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000044_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000047.mp4 ---\n",
      "'0922_squat_000047.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000047.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000047.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000047_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000081.mp4 ---\n",
      "'0922_squat_000081.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000081.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000081.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000081_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000082.mp4 ---\n",
      "'0922_squat_000082.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000082.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000082.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000082_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000084.mp4 ---\n",
      "'0922_squat_000084.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000084.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000084.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000084_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000085.mp4 ---\n",
      "'0922_squat_000085.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000085.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000085.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000085_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000086.mp4 ---\n",
      "'0922_squat_000086.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000086.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000086.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000086_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000088.mp4 ---\n",
      "'0922_squat_000088.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000088.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000088.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000088_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000139.mp4 ---\n",
      "'0922_squat_000139.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000139.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000139.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000139_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000140.mp4 ---\n",
      "'0922_squat_000140.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000140.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000140.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000140_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000141.mp4 ---\n",
      "'0922_squat_000141.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000141.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000141.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000141_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000144.mp4 ---\n",
      "'0922_squat_000144.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000144.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000144.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000144_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000180.mp4 ---\n",
      "'0922_squat_000180.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000180.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000180.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000180_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000181.mp4 ---\n",
      "'0922_squat_000181.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000181.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000181.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000181_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000182.mp4 ---\n",
      "'0922_squat_000182.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000182.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000182.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000182_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000184.mp4 ---\n",
      "'0922_squat_000184.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000184.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000184.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000184_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0922_squat_000185.mp4 ---\n",
      "'0922_squat_000185.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0922_squat_000185.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0922_squat_000185.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0922_squat_000185_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0926_squat_000042.mp4 ---\n",
      "'0926_squat_000042.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0926_squat_000042.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0926_squat_000042.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0926_squat_000042_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0926_squat_000043.mp4 ---\n",
      "'0926_squat_000043.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0926_squat_000043.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0926_squat_000043.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0926_squat_000043_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0926_squat_000049.mp4 ---\n",
      "'0926_squat_000049.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0926_squat_000049.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0926_squat_000049.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0926_squat_000049_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0926_squat_000050.mp4 ---\n",
      "'0926_squat_000050.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0926_squat_000050.mp4' 처리 완료 (총 300 프레임) 또는 프레임 읽기 실패.\n",
      "'0926_squat_000050.mp4' 최종 데이터 형태: (300, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0926_squat_000050_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0926_squat_000144.mp4 ---\n",
      "'0926_squat_000144.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n",
      "'0926_squat_000144.mp4' 처리 완료 (총 299 프레임) 또는 프레임 읽기 실패.\n",
      "'0926_squat_000144.mp4' 최종 데이터 형태: (299, 12, 3)\n",
      "선택된 랜드마크 데이터 저장 완료: ./../data/mediapipe_res/bad_innner_thigh/0926_squat_000144_res.npy\n",
      "\n",
      "--- 비디오 처리 시작: /data/hamboong/tmp/cap/data/Video_Dataset/bad_innner_thigh/1115_video/0926_squat_000147.mp4 ---\n",
      "'0926_squat_000147.mp4' 처리 중... (선택된 12개 랜드마크 저장)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# 성능 향상을 위해 이미지를 쓰기 불가로 전환 (선택 사항)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m image_rgb\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 이미지 다시 쓰기 가능하게 전환\u001b[39;00m\n\u001b[1;32m     99\u001b[0m image_rgb\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/data/hamboong/hj/lib/python3.10/site-packages/mediapipe/python/solutions/pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[0;32m/data/hamboong/hj/lib/python3.10/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import glob # 파일 목록을 가져오기 위해 추가\n",
    "\n",
    "# --- 설정 ---\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 저장할 랜드마크 인덱스 리스트 정의\n",
    "# 11:L_SHOULDER, 12:R_SHOULDER, 23:L_HIP, 24:R_HIP, 25:L_KNEE, 26:R_KNEE,\n",
    "# 27:L_ANKLE, 28:R_ANKLE, 29:L_HEEL, 30:R_HEEL, 31:L_FOOT_INDEX, 32:R_FOOT_INDEX\n",
    "landmark_indices_to_save = [11, 12, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
    "num_selected_landmarks = len(landmark_indices_to_save) # 저장할 랜드마크 개수 (12개)\n",
    "\n",
    "# 비디오 파일이 있는 디렉토리 경로 *** 수정된 부분 ***\n",
    "input_video_dir = './../data/Video_Dataset/good' # 예시: 비디오들이 있는 폴더\n",
    "\n",
    "# 처리할 비디오 파일 확장자 목록 *** 추가된 부분 ***\n",
    "# 필요에 따라 '.avi', '.mov' 등을 추가하세요.\n",
    "allowed_extensions = ('.mp4', '.avi', '.mov', '.mkv')\n",
    "# 결과를 저장할 디렉토리 경로\n",
    "# output_dir = './../data/mediapipe_res/'\n",
    "output_dir = './../data/mediapipe_res/good'\n",
    "\n",
    "# 화면 표시용 창 크기 조절 비율 (0.5 = 50%) - 주석 처리 (필요시 해제)\n",
    "# display_scale = 0.5\n",
    "\n",
    "# --- MediaPipe Pose 객체 생성 (한 번만 생성) ---\n",
    "pose = None # 초기화\n",
    "try:\n",
    "    pose = mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5)\n",
    "    print(\"MediaPipe Pose 객체 생성 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"MediaPipe Pose 객체 생성 중 오류 발생: {e}\")\n",
    "    exit() # Pose 객체 생성 실패 시 종료\n",
    "\n",
    "# --- 저장 디렉토리 생성 (한 번만 실행) ---\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"결과 저장 디렉토리: {output_dir}\")\n",
    "\n",
    "# --- 지정된 디렉토리의 모든 비디오 파일 처리 --- *** 수정된 부분 ***\n",
    "\n",
    "# input_video_dir 내의 모든 파일을 검색\n",
    "all_files = os.listdir(input_video_dir)\n",
    "video_files = [f for f in all_files if os.path.isfile(os.path.join(input_video_dir, f)) and f.lower().endswith(allowed_extensions)]\n",
    "\n",
    "print(f\"\\n총 {len(video_files)}개의 비디오 파일을 처리합니다: {video_files}\")\n",
    "\n",
    "if not video_files:\n",
    "    print(f\"오류: '{input_video_dir}' 디렉토리에서 비디오 파일을 찾을 수 없습니다.\")\n",
    "    # pose 객체가 생성되었는지 확인 후 닫기 (오류 방지)\n",
    "    if pose:\n",
    "        try:\n",
    "            pose.close()\n",
    "            print(\"MediaPipe Pose 객체를 닫았습니다 (비디오 없음).\")\n",
    "        except Exception as e:\n",
    "             print(f\"MediaPipe Pose 객체를 닫는 중 오류 발생 (비디오 없음): {e}\")\n",
    "    exit()\n",
    "\n",
    "# 각 비디오 파일에 대해 루프 실행\n",
    "for video_filename in video_files:\n",
    "    video_path = os.path.join(input_video_dir, video_filename)\n",
    "    print(f\"\\n--- 비디오 처리 시작: {video_path} ---\")\n",
    "\n",
    "    # --- 비디오 처리 및 데이터 저장 (각 비디오별로 실행) ---\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"오류: 비디오 파일을 열 수 없습니다 -> {video_path}. 다음 파일로 건너<0xEB><0x9B><0x84>니다.\")\n",
    "        continue # 다음 비디오 파일로 넘어감\n",
    "\n",
    "    all_frame_landmarks = []\n",
    "    frame_count = 0\n",
    "    print(f\"'{video_filename}' 처리 중... (선택된 {num_selected_landmarks}개 랜드마크 저장)\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(f\"'{video_filename}' 처리 완료 (총 {frame_count} 프레임) 또는 프레임 읽기 실패.\")\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        # 원본 이미지 크기는 필요시 사용 (화면 표시 등)\n",
    "        # original_height, original_width = image.shape[:2]\n",
    "\n",
    "        # MediaPipe 처리\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # 성능 향상을 위해 이미지를 쓰기 불가로 전환 (선택 사항)\n",
    "        image_rgb.flags.writeable = False\n",
    "        results = pose.process(image_rgb)\n",
    "        # 이미지 다시 쓰기 가능하게 전환\n",
    "        image_rgb.flags.writeable = True\n",
    "        # annotated_image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR) # 시각화 시 필요\n",
    "\n",
    "        # 현재 프레임의 *선택된* 랜드마크 데이터를 저장할 NumPy 배열\n",
    "        # 크기: (num_selected_landmarks, 3), 기본값은 NaN\n",
    "        frame_landmarks_np = np.full((num_selected_landmarks, 3), np.nan, dtype=np.float32)\n",
    "\n",
    "        if results.pose_world_landmarks:\n",
    "            landmarks = results.pose_world_landmarks.landmark\n",
    "            # 지정된 인덱스의 랜드마크 좌표만 추출하여 배열 생성\n",
    "            try:\n",
    "                 # 리스트 컴프리헨션을 사용하여 선택된 인덱스의 좌표만 추출\n",
    "                 selected_coords = [\n",
    "                     [landmarks[i].x, landmarks[i].y, landmarks[i].z]\n",
    "                     for i in landmark_indices_to_save\n",
    "                 ]\n",
    "                 # 좌표값이 유효한지 확인 (가끔 비정상적인 값이 들어올 수 있음)\n",
    "                 if all(-1e6 < coord < 1e6 for point in selected_coords for coord in point):\n",
    "                    frame_landmarks_np = np.array(selected_coords, dtype=np.float32)\n",
    "                 else:\n",
    "                    print(f\"프레임 {frame_count} ({video_filename}): 비정상적인 랜드마크 좌표 감지. NaN으로 처리.\")\n",
    "                    # frame_landmarks_np는 이미 NaN으로 초기화되어 있음\n",
    "\n",
    "            except IndexError:\n",
    "                 print(f\"프레임 {frame_count} ({video_filename}): 랜드마크 인덱스 접근 오류 발생. NaN으로 처리.\")\n",
    "                 # frame_landmarks_np는 이미 NaN으로 초기화되어 있음\n",
    "            except Exception as e:\n",
    "                 print(f\"프레임 {frame_count} ({video_filename}): 랜드마크 처리 중 오류 발생: {e}. NaN으로 처리.\")\n",
    "\n",
    "            # (선택 사항) 시각화 - 주석 처리\n",
    "            # if results.pose_landmarks:\n",
    "            #     mp_drawing.draw_landmarks(\n",
    "            #         annotated_image,\n",
    "            #         results.pose_landmarks, # 여기는 여전히 모든 랜드마크 표시\n",
    "            #         mp_pose.POSE_CONNECTIONS,\n",
    "            #         landmark_drawing_spec=mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "            #         connection_drawing_spec=mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "            #     )\n",
    "\n",
    "        # 현재 프레임의 *선택된* 랜드마크 배열을 전체 리스트에 추가\n",
    "        all_frame_landmarks.append(frame_landmarks_np)\n",
    "\n",
    "        # (선택 사항) 리사이즈 및 화면 표시 - 주석 처리\n",
    "        # new_width = int(original_width * display_scale)\n",
    "        # new_height = int(original_height * display_scale)\n",
    "        # resized_image = cv2.resize(annotated_image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "        # cv2.imshow(f'MediaPipe Pose - {video_filename}', resized_image)\n",
    "        # if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        #     print(\"사용자 요청으로 처리 중단.\")\n",
    "        #     break # 내부 루프 중단\n",
    "\n",
    "    # --- 한 비디오 루프 종료 후 데이터 저장 ---\n",
    "    cap.release() # 현재 비디오 파일 닫기\n",
    "    # cv2.destroyAllWindows() # 각 비디오마다 창을 닫으려면 여기, 모든 처리 후 닫으려면 밖으로 이동\n",
    "\n",
    "    if all_frame_landmarks:\n",
    "        # 리스트를 하나의 큰 3D NumPy 배열로 결합\n",
    "        # 최종 형태: (총 프레임 수, num_selected_landmarks, 3) -> 예: (183, 12, 3)\n",
    "        try:\n",
    "            landmark_data_np = np.array(all_frame_landmarks)\n",
    "            print(f\"'{video_filename}' 최종 데이터 형태: {landmark_data_np.shape}\")\n",
    "\n",
    "            # 저장 파일명 생성 (원본 비디오 파일명 기반)\n",
    "            base_filename = os.path.splitext(video_filename)[0]\n",
    "            output_filename = f\"{base_filename}_res.npy\"\n",
    "            output_filepath = os.path.join(output_dir, output_filename)\n",
    "\n",
    "            # NumPy 배열 저장\n",
    "            np.save(output_filepath, landmark_data_np)\n",
    "            print(f\"선택된 랜드마크 데이터 저장 완료: {output_filepath}\")\n",
    "\n",
    "        except ValueError as ve:\n",
    "             print(f\"'{video_filename}' 데이터 배열 변환 중 오류 발생: {ve}. 프레임별 랜드마크 개수가 다를 수 있습니다. 건너<0xEB><0x9B><0x84>니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"'{output_filepath}' 파일 저장 또는 데이터 처리 중 오류 발생: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"'{video_filename}'에서 저장할 랜드마크 데이터가 없습니다.\")\n",
    "\n",
    "    # 'q' 키로 중단된 경우 외부 루프도 중단 (필요시 주석 해제)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'): # waitKey(5) 루프 안에 있을 때만 유효\n",
    "    #     break\n",
    "\n",
    "# --- 모든 비디오 처리 완료 후 ---\n",
    "# cv2.destroyAllWindows() # 모든 창 닫기 (주석처리 되어있음)\n",
    "\n",
    "# MediaPipe Pose 객체 닫기 (객체가 유효할 때만) *** 수정된 부분 ***\n",
    "if pose:\n",
    "    try:\n",
    "        pose.close()\n",
    "        print(\"\\nMediaPipe Pose 객체를 성공적으로 닫았습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nMediaPipe Pose 객체를 닫는 중 오류 발생: {e}\")\n",
    "else:\n",
    "    print(\"\\nMediaPipe Pose 객체가 유효하지 않아 닫기를 건너<0xEB><0x9B><0x84>니다.\")\n",
    "\n",
    "\n",
    "print(\"\\n모든 비디오 파일 처리 완료.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603090f4",
   "metadata": {},
   "source": [
    "# npy파일 읽어오기 (임시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95de343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 로드 성공: ./../data/mediapipe_res/squat_res.npy\n",
      "로드된 데이터 형태: (183, 12, 3)\n",
      "[[[-0.15581077 -0.46270552 -0.04443497]\n",
      "  [ 0.16361548 -0.46307558 -0.0032042 ]\n",
      "  [-0.08693165 -0.00426618  0.00285172]\n",
      "  ...\n",
      "  [ 0.25140098  0.7802632   0.05736061]\n",
      "  [-0.22107351  0.7339341   0.03744415]\n",
      "  [ 0.2957428   0.7685144   0.0658367 ]]\n",
      "\n",
      " [[-0.1557893  -0.46018198 -0.06593481]\n",
      "  [ 0.16423976 -0.46305236 -0.023705  ]\n",
      "  [-0.08739546 -0.0035474   0.0046195 ]\n",
      "  ...\n",
      "  [ 0.24797574  0.770414    0.12350331]\n",
      "  [-0.23832434  0.7117693   0.11575799]\n",
      "  [ 0.29508886  0.75143105  0.13343483]]\n",
      "\n",
      " [[-0.15676185 -0.45699316 -0.05936539]\n",
      "  [ 0.16465895 -0.45922515 -0.01701877]\n",
      "  [-0.08946616 -0.00330456  0.00541176]\n",
      "  ...\n",
      "  [ 0.24844845  0.7410742   0.14707631]\n",
      "  [-0.2383831   0.69079995  0.15047199]\n",
      "  [ 0.2950963   0.7245745   0.16787218]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.1363099  -0.4344412  -0.05622086]\n",
      "  [ 0.18938147 -0.41922623 -0.03813142]\n",
      "  [-0.1027734  -0.00108946  0.01134231]\n",
      "  ...\n",
      "  [ 0.2814197   0.2612016   0.3966929 ]\n",
      "  [-0.29609832  0.24099651  0.52298594]\n",
      "  [ 0.30974445  0.2801383   0.48074096]]\n",
      "\n",
      " [[-0.13545205 -0.4348313  -0.05549877]\n",
      "  [ 0.18808392 -0.41975877 -0.03697397]\n",
      "  [-0.1016157  -0.00123195  0.01118609]\n",
      "  ...\n",
      "  [ 0.28086904  0.27632526  0.38933784]\n",
      "  [-0.29514936  0.24838673  0.51871574]\n",
      "  [ 0.30446163  0.2911506   0.46995702]]\n",
      "\n",
      " [[-0.13570282 -0.43587524 -0.05520158]\n",
      "  [ 0.18786247 -0.4212357  -0.03627701]\n",
      "  [-0.10063415 -0.00128649  0.01091244]\n",
      "  ...\n",
      "  [ 0.2789792   0.28594282  0.38636896]\n",
      "  [-0.2948051   0.2608773   0.4931413 ]\n",
      "  [ 0.2990523   0.2985554   0.47074434]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 저장된 파일 경로 (위 코드에서 저장한 경로와 동일해야 함)\n",
    "output_dir = './../data/mediapipe_res/'\n",
    "video_basename = 'squat' # 예시 원본 파일명 (확장자 제외)\n",
    "filename = f\"{video_basename}_res.npy\"\n",
    "filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "# .npy 파일 로드\n",
    "try:\n",
    "    loaded_landmarks = np.load(filepath)\n",
    "    print(f\"파일 로드 성공: {filepath}\")\n",
    "    print(f\"로드된 데이터 형태: {loaded_landmarks.shape}\") # 예: (183, 12, 3)\n",
    "\n",
    "    print(loaded_landmarks)\n",
    "    # # 로드된 데이터 확인\n",
    "    # # 첫 번째 프레임의 첫 번째 저장된 랜드마크(LEFT_SHOULDER, 원래 인덱스 11) 좌표\n",
    "    # if loaded_landmarks.shape[0] > 0:\n",
    "    #     print(\"\\n첫 번째 프레임 데이터 예시 (일부):\")\n",
    "    #     # 저장된 배열의 0번 인덱스 = 원래 11번(LEFT_SHOULDER)\n",
    "    #     print(f\"  프레임 0, 저장된 0번 랜드마크(원래 11번): {loaded_landmarks[0, 0, :]}\")\n",
    "    #     # 저장된 배열의 2번 인덱스 = 원래 23번(LEFT_HIP)\n",
    "    #     print(f\"  프레임 0, 저장된 2번 랜드마크(원래 23번): {loaded_landmarks[0, 2, :]}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: 파일을 찾을 수 없습니다 -> {filepath}\")\n",
    "except Exception as e:\n",
    "    print(f\"파일 로드 중 오류 발생: {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbdfc8",
   "metadata": {},
   "source": [
    "# 파생 Feature 생성 (10개)\n",
    "\n",
    "- **관절 각도:**\n",
    "    1. **왼쪽 무릎 각도 (Left Knee Angle):** 왼쪽 엉덩이-왼쪽 무릎-왼쪽 발목 사이의 각도\n",
    "    2. **오른쪽 무릎 각도 (Right Knee Angle):** 오른쪽 엉덩이-오른쪽 무릎-오른쪽 발목 사이의 각도\n",
    "    3. **왼쪽 고관절 각도 (Left Hip Angle):** 왼쪽 어깨-왼쪽 엉덩이-왼쪽 무릎 사이의 각도\n",
    "    4. **오른쪽 고관절 각도 (Right Hip Angle):** 오른쪽 어깨-오른쪽 엉덩이-오른쪽 무릎 사이의 각도\n",
    "    5. **왼쪽 발목 각도 (Left Ankle Angle):** 왼쪽 무릎-왼쪽 발목-왼쪽 발끝 사이의 각도\n",
    "    6. **오른쪽 발목 각도 (Right Ankle Angle):** 오른쪽 무릎-오른쪽 발목-오른쪽 발끝 사이의 각도\n",
    "    7. **상체 기울기 (Torso Angle):** 엉덩이 중앙과 어깨 중앙을 잇는 선이 수직선(Y축)과 이루는 각도. (앞으로 얼마나 숙였는지)\n",
    "\n",
    "- **상대적 위치/거리:**\n",
    "\n",
    "    8. **엉덩이 높이 (Hip Height):** 발목(또는 발뒤꿈치) 중앙으로부터 엉덩이 중앙까지의 수직(Y) 거리. (스쿼트 깊이)\n",
    "    9. **무릎 간 거리 (Knee Distance):** 왼쪽 무릎과 오른쪽 무릎 사이의 수평 거리. (무릎 모임 등 확인)\n",
    "    10. **발목 간 거리 (Ankle Distance / Stance Width):** 왼쪽 발목과 오른쪽 발목 사이의 수평 거리. (스탠스 너비)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80749d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오류: 예시 데이터 파일(./data/mediapipe_res/0918_squat_000001_res.npy)을 찾을 수 없습니다. \n",
      "먼저 랜드마크 저장 코드를 실행하여 .npy 파일을 생성해주세요.\n",
      "임시 더미 데이터로 특징 추출 테스트를 진행합니다.\n",
      "로드된 랜드마크 데이터 형태: (300, 12, 3)\n",
      "\n",
      "최종 파생 특징 데이터 형태: (300, 10)\n",
      "[[112.084526    31.780703   116.0378     ...   0.36453608   0.81620485\n",
      "    0.9974435 ]\n",
      " [ 79.33784     45.571724    97.18631    ...   0.52613163   0.83155316\n",
      "    0.35017005]\n",
      " [ 52.423473    29.941498    63.05344    ...   0.23798396   0.6449251\n",
      "    1.0046434 ]\n",
      " ...\n",
      " [ 87.97786     60.82125     78.70274    ...   0.15367049   0.82493156\n",
      "    0.66171145]\n",
      " [ 93.20865     34.481865    55.538914   ...   0.2849288    0.18677577\n",
      "    0.13462302]\n",
      " [ 22.758097    13.43904     50.10211    ...   0.1245476    0.73259896\n",
      "    0.83435   ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"세 점 a, b, c가 주어졌을 때 점 b에서의 각도를 계산 (단위: 도)\"\"\"\n",
    "    a = np.array(a)  # 첫 번째 점\n",
    "    b = np.array(b)  # 중간 점 (각도의 꼭짓점)\n",
    "    c = np.array(c)  # 세 번째 점\n",
    "\n",
    "    # 벡터 BA, BC 계산\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "\n",
    "    # 코사인 각도 계산\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "\n",
    "    # np.arccos 입력값은 [-1, 1] 범위여야 함 (부동소수점 오류 방지)\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def extract_derived_features(landmarks_frame):\n",
    "    \"\"\"\n",
    "    한 프레임의 선택된 랜드마크 (12, 3) 배열로부터 파생 특징을 계산\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    # 입력 랜드마크 배열에서 각 부위 좌표 할당\n",
    "    try:\n",
    "        l_shoulder = landmarks_frame[0]\n",
    "        r_shoulder = landmarks_frame[1]\n",
    "        l_hip = landmarks_frame[2]\n",
    "        r_hip = landmarks_frame[3]\n",
    "        l_knee = landmarks_frame[4]\n",
    "        r_knee = landmarks_frame[5]\n",
    "        l_ankle = landmarks_frame[6]\n",
    "        r_ankle = landmarks_frame[7]\n",
    "        # l_heel = landmarks_frame[8] # 필요시 사용\n",
    "        # r_heel = landmarks_frame[9] # 필요시 사용\n",
    "        l_foot_index = landmarks_frame[10]\n",
    "        r_foot_index = landmarks_frame[11]\n",
    "\n",
    "        # 모든 입력 랜드마크가 유효한지 (NaN이 아닌지) 확인\n",
    "        if np.isnan(landmarks_frame).any():\n",
    "            # 랜드마크 중 하나라도 NaN이면 10개의 NaN 특징 반환\n",
    "            return np.full(10, np.nan, dtype=np.float32)\n",
    "\n",
    "        # 1. 왼쪽 무릎 각도\n",
    "        left_knee_angle = calculate_angle(l_hip, l_knee, l_ankle)\n",
    "        features.append(left_knee_angle)\n",
    "\n",
    "        # 2. 오른쪽 무릎 각도\n",
    "        right_knee_angle = calculate_angle(r_hip, r_knee, r_ankle)\n",
    "        features.append(right_knee_angle)\n",
    "\n",
    "        # 3. 왼쪽 고관절 각도\n",
    "        left_hip_angle = calculate_angle(l_shoulder, l_hip, l_knee)\n",
    "        features.append(left_hip_angle)\n",
    "\n",
    "        # 4. 오른쪽 고관절 각도\n",
    "        right_hip_angle = calculate_angle(r_shoulder, r_hip, r_knee)\n",
    "        features.append(right_hip_angle)\n",
    "\n",
    "        # 5. 왼쪽 발목 각도\n",
    "        left_ankle_angle = calculate_angle(l_knee, l_ankle, l_foot_index)\n",
    "        features.append(left_ankle_angle)\n",
    "\n",
    "        # 6. 오른쪽 발목 각도\n",
    "        right_ankle_angle = calculate_angle(r_knee, r_ankle, r_foot_index)\n",
    "        features.append(right_ankle_angle)\n",
    "\n",
    "        # 7. 상체 기울기 (엉덩이 중앙-어깨 중앙 벡터와 수직선(Y축) 사이의 각도)\n",
    "        mid_shoulder = (l_shoulder + r_shoulder) / 2\n",
    "        mid_hip = (l_hip + r_hip) / 2\n",
    "        torso_vector = mid_shoulder - mid_hip # 엉덩이 -> 어깨 방향 벡터\n",
    "        \n",
    "        # Y축 방향 벡터 (0,1,0) - MediaPipe 월드 좌표계 기준 Y축이 위쪽\n",
    "        # 만약 Y축이 아래쪽을 향한다면 (0,-1,0) 또는 각도 계산 후 보정 필요\n",
    "        vertical_vector = np.array([0, 1, 0]) \n",
    "        \n",
    "        # torso_vector가 거의 0벡터이거나 vertical_vector가 0벡터일 가능성은 낮으나 안전하게 처리\n",
    "        norm_torso = np.linalg.norm(torso_vector)\n",
    "        norm_vertical = np.linalg.norm(vertical_vector) # 항상 1\n",
    "\n",
    "        if norm_torso > 1e-6 : # 매우 작은 벡터는 각도 계산 무의미\n",
    "            cosine_torso_vertical = np.dot(torso_vector, vertical_vector) / (norm_torso * norm_vertical)\n",
    "            torso_angle = np.degrees(np.arccos(np.clip(cosine_torso_vertical, -1.0, 1.0)))\n",
    "        else:\n",
    "            torso_angle = np.nan # 또는 90도 등 적절한 값\n",
    "        features.append(torso_angle) # 0도에 가까울수록 상체가 수직으로 선 상태\n",
    "\n",
    "        # 8. 엉덩이 높이 (발목 중앙 기준 Y좌표 차이)\n",
    "        mid_ankle = (l_ankle + r_ankle) / 2\n",
    "        hip_height = mid_hip[1] - mid_ankle[1] # Y 좌표 차이\n",
    "        features.append(hip_height)\n",
    "\n",
    "        # 9. 무릎 간 거리 (X, Z 평면에서의 거리)\n",
    "        knee_diff_vector = l_knee - r_knee\n",
    "        knee_distance = np.linalg.norm(np.array([knee_diff_vector[0], knee_diff_vector[2]])) # X, Z 성분만 사용\n",
    "        features.append(knee_distance)\n",
    "\n",
    "        # 10. 발목 간 거리 (스탠스 너비, X, Z 평면에서의 거리)\n",
    "        ankle_diff_vector = l_ankle - r_ankle\n",
    "        ankle_distance = np.linalg.norm(np.array([ankle_diff_vector[0], ankle_diff_vector[2]])) # X, Z 성분만 사용\n",
    "        features.append(ankle_distance)\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"특징 계산 중 오류: {e}\")\n",
    "        # 오류 발생 시 NaN으로 채워진 특징 배열 반환\n",
    "        return np.full(10, np.nan, dtype=np.float32)\n",
    "        \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "if __name__ == '__main__':\n",
    "    # 예시: 하나의 .npy 파일 로드 (실제 경로로 변경 필요)\n",
    "    data_dir = './data/mediapipe_res/'\n",
    "    video_filename = '0918_squat_000001_res.npy' # 이전 코드에서 저장한 파일명 예시\n",
    "    full_path = os.path.join(data_dir, video_filename)\n",
    "\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"오류: 예시 데이터 파일({full_path})을 찾을 수 없습니다. \")\n",
    "        print(\"먼저 랜드마크 저장 코드를 실행하여 .npy 파일을 생성해주세요.\")\n",
    "        # 임시 더미 데이터 생성으로 테스트 계속 진행\n",
    "        print(\"임시 더미 데이터로 특징 추출 테스트를 진행합니다.\")\n",
    "        video_landmarks_data = np.random.rand(300, 12, 3).astype(np.float32) # (프레임, 랜드마크, 좌표)\n",
    "    else:\n",
    "        video_landmarks_data = np.load(full_path) # (300, 12, 3) 형태\n",
    "\n",
    "    print(f\"로드된 랜드마크 데이터 형태: {video_landmarks_data.shape}\")\n",
    "\n",
    "    all_frames_derived_features = []\n",
    "    for frame_idx in range(video_landmarks_data.shape[0]):\n",
    "        landmarks_one_frame = video_landmarks_data[frame_idx, :, :] # (12, 3)\n",
    "        \n",
    "        # 현재 프레임의 랜드마크가 모두 NaN인지 확인 (랜드마크 감지 실패 프레임)\n",
    "        if np.isnan(landmarks_one_frame).all():\n",
    "            derived_features = np.full(10, np.nan, dtype=np.float32) # 특징 개수에 맞게 NaN 배열 생성\n",
    "            # print(f\"프레임 {frame_idx}: 랜드마크 데이터가 없어 특징을 NaN으로 처리합니다.\")\n",
    "        else:\n",
    "            derived_features = extract_derived_features(landmarks_one_frame)\n",
    "            # print(f\"프레임 {frame_idx} 특징: {derived_features}\")\n",
    "        \n",
    "        all_frames_derived_features.append(derived_features)\n",
    "\n",
    "    # 리스트를 NumPy 배열로 변환 (최종 형태: [총 프레임 수, 파생 특징 수])\n",
    "    derived_features_np = np.array(all_frames_derived_features)\n",
    "    print(f\"\\n최종 파생 특징 데이터 형태: {derived_features_np.shape}\") # 예: (300, 10)\n",
    "    \n",
    "    # NaN 값 개수 확인 (선택 사항)\n",
    "    nan_count = np.isnan(derived_features_np).sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"파생 특징 데이터에 총 {nan_count}개의 NaN 값이 포함되어 있습니다.\")\n",
    "        # NaN 값 처리 방법 고려 필요 (예: 평균값 대체, 해당 윈도우 제외 등)\n",
    "\n",
    "    # 이제 이 derived_features_np 배열을 가지고 시퀀스 윈도우를 만들고\n",
    "    # LSTM Autoencoder 학습 데이터로 사용할 수 있습니다.\n",
    "    # (예: (300,10) -> 여러 개의 (30,10) 윈도우로 변환)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d819c21",
   "metadata": {},
   "source": [
    "# Model Train\n",
    "\n",
    "- Model : Lstm Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c2eb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "42cc3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. 파생 특징 계산  ---\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"세 점 a, b, c가 주어졌을 때 점 b에서의 각도를 계산 (단위: 도)\"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6) # 분모 0 방지\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def extract_derived_features(landmarks_frame):\n",
    "    \"\"\"한 프레임의 선택된 랜드마크 (12, 3) 배열로부터 파생 특징을 계산\"\"\"\n",
    "    features = []\n",
    "    try:\n",
    "        l_shoulder, r_shoulder, l_hip, r_hip, l_knee, r_knee, \\\n",
    "        l_ankle, r_ankle, _, _, l_foot_index, r_foot_index = landmarks_frame\n",
    "\n",
    "        if np.isnan(landmarks_frame).any():\n",
    "            return np.full(10, np.nan, dtype=np.float32)\n",
    "\n",
    "        features.append(calculate_angle(l_hip, l_knee, l_ankle))\n",
    "        features.append(calculate_angle(r_hip, r_knee, r_ankle))\n",
    "        features.append(calculate_angle(l_shoulder, l_hip, l_knee))\n",
    "        features.append(calculate_angle(r_shoulder, r_hip, r_knee))\n",
    "        features.append(calculate_angle(l_knee, l_ankle, l_foot_index))\n",
    "        features.append(calculate_angle(r_knee, r_ankle, r_foot_index))\n",
    "\n",
    "        mid_shoulder = (l_shoulder + r_shoulder) / 2\n",
    "        mid_hip = (l_hip + r_hip) / 2\n",
    "        torso_vector = mid_shoulder - mid_hip\n",
    "        vertical_vector = np.array([0, 1, 0])\n",
    "        norm_torso = np.linalg.norm(torso_vector)\n",
    "        if norm_torso > 1e-6:\n",
    "            cosine_torso_vertical = np.dot(torso_vector, vertical_vector) / norm_torso\n",
    "            features.append(np.degrees(np.arccos(np.clip(cosine_torso_vertical, -1.0, 1.0))))\n",
    "        else:\n",
    "            features.append(np.nan)\n",
    "\n",
    "        mid_ankle = (l_ankle + r_ankle) / 2\n",
    "        features.append(mid_hip[1] - mid_ankle[1]) # Y좌표 기준 엉덩이 높이\n",
    "\n",
    "        knee_diff_vector = l_knee - r_knee\n",
    "        features.append(np.linalg.norm(np.array([knee_diff_vector[0], knee_diff_vector[2]])))\n",
    "\n",
    "        ankle_diff_vector = l_ankle - r_ankle\n",
    "        features.append(np.linalg.norm(np.array([ankle_diff_vector[0], ankle_diff_vector[2]])))\n",
    "    except Exception:\n",
    "        return np.full(10, np.nan, dtype=np.float32)\n",
    "    return np.array(features, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2af7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 하이퍼파라미터 및 설정 ---\n",
    "DATA_DIR = \"./../data/mediapipe_res/good\" # .npy 파일들이 있는 디렉토리\n",
    "TIME_STEPS = 30          # 윈도우 크기 (입력 시퀀스 길이)\n",
    "N_DERIVED_FEATURES = 10  # extract_derived_features 함수가 반환하는 특징 개수\n",
    "LATENT_DIM = 8           # 잠재 공간 차원\n",
    "HIDDEN_DEC_DIM = 64      # 디코더 LSTM의 은닉 차원\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 50 # 예시, 실제로는 더 많이 필요할 수 있음\n",
    "WINDOW_STRIDE = 1 # 슬라이딩 윈도우 간격\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5ce6206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN 포함 윈도우 제거 후 총 윈도우 수: 71339\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 데이터 로딩 및 전처리 ---\n",
    "def load_and_preprocess_data(data_dir, feature_extraction_fn, time_steps, stride):\n",
    "    all_sequences = []\n",
    "    npy_files = glob.glob(os.path.join(data_dir, \"*.npy\"))\n",
    "    if not npy_files:\n",
    "        print(f\"경고: {data_dir} 에서 .npy 파일을 찾을 수 없습니다.\")\n",
    "        return np.array([]), None\n",
    "\n",
    "    for npy_file in tqdm(npy_files, leave=False, desc=\"processing data\"):\n",
    "        try:\n",
    "            landmarks_video = np.load(npy_file) # (300, 12, 3)\n",
    "            if landmarks_video.ndim != 3 or landmarks_video.shape[1] != 12 or landmarks_video.shape[2] != 3:\n",
    "                print(f\"경고: {npy_file} 파일의 형태가 올바르지 않습니다 ({landmarks_video.shape}). \")\n",
    "                continue\n",
    "            \n",
    "            derived_features_video = []\n",
    "            for frame_idx in range(landmarks_video.shape[0]):\n",
    "                landmarks_frame = landmarks_video[frame_idx, :, :]\n",
    "                derived_features = feature_extraction_fn(landmarks_frame)\n",
    "                derived_features_video.append(derived_features)\n",
    "            \n",
    "            derived_features_video = np.array(derived_features_video) # (300, N_DERIVED_FEATURES)\n",
    "\n",
    "            # 슬라이딩 윈도우 생성\n",
    "            for i in range(0, derived_features_video.shape[0] - time_steps + 1, stride):\n",
    "                sequence = derived_features_video[i : i + time_steps]\n",
    "                all_sequences.append(sequence)\n",
    "        except Exception as e:\n",
    "            print(f\"{npy_file} 처리 중 오류 발생: {e}\")\n",
    "\n",
    "    if not all_sequences:\n",
    "        print(\"처리할 시퀀스가 없습니다.\")\n",
    "        return np.array([]), None\n",
    "        \n",
    "    all_sequences_np = np.array(all_sequences, dtype=np.float32) # (num_windows, time_steps, n_derived_features)\n",
    "    \n",
    "    # NaN 값을 포함하는 윈도우 제거\n",
    "    nan_mask = ~np.isnan(all_sequences_np).any(axis=(1, 2))\n",
    "    all_sequences_np = all_sequences_np[nan_mask]\n",
    "    print(f\"NaN 포함 윈도우 제거 후 총 윈도우 수: {all_sequences_np.shape[0]}\")\n",
    "\n",
    "    if all_sequences_np.shape[0] == 0:\n",
    "        print(\"NaN 제거 후 남은 윈도우가 없습니다.\")\n",
    "        return np.array([]), None\n",
    "\n",
    "    # 데이터 정규화 (StandardScaler)\n",
    "    # 스케일러는 학습 데이터에만 fit하고, 전체 데이터에 transform 적용\n",
    "    # 여기서는 전체 데이터를 한번에 로드했으므로, 분할 후 스케일링 필요.\n",
    "    # 혹은 모든 정상 데이터로 스케일러를 학습시킬 수도 있음 (오토인코더의 경우)\n",
    "    # 여기서는 편의상 전체에 대해 한번에 하지만, 엄밀하게는 train/val 분리 후 train으로 fit해야 함.\n",
    "    \n",
    "    # scaler를 반환하기 위해 일단 여기서는 fit/transform 하지 않고, 데이터를 반환\n",
    "    return all_sequences_np\n",
    "\n",
    "# 데이터 로드\n",
    "raw_windows = load_and_preprocess_data(DATA_DIR, extract_derived_features, TIME_STEPS, WINDOW_STRIDE)\n",
    "\n",
    "# if raw_windows.shape[0] < 2 : # 학습/검증 분할을 위해 최소 2개 필요\n",
    "#     print(\"학습을 위한 데이터가 부족합니다. 프로그램을 종료합니다.\")\n",
    "#     exit()\n",
    "\n",
    "# 학습/검증 데이터 분할 (예: 80% 학습, 20% 검증)\n",
    "train_size = int(0.8 * len(raw_windows))\n",
    "val_size = len(raw_windows) - train_size\n",
    "train_data, val_data = random_split(raw_windows, [train_size, val_size])\n",
    "\n",
    "# 정규화\n",
    "# StandardScaler는 2D 데이터를 기대하므로 reshape 필요\n",
    "scaler = StandardScaler()\n",
    "# train_data를 2D로 변환하여 scaler 학습\n",
    "n_samples_train, ts_train, n_features_train = train_data.dataset[train_data.indices].shape # PyTorch Subset의 실제 데이터 접근\n",
    "train_data_reshaped = train_data.dataset[train_data.indices].reshape(-1, n_features_train)\n",
    "scaler.fit(train_data_reshaped)\n",
    "\n",
    "# 학습 데이터 정규화\n",
    "train_data_scaled_reshaped = scaler.transform(train_data_reshaped)\n",
    "train_sequences = train_data_scaled_reshaped.reshape(n_samples_train, ts_train, n_features_train)\n",
    "\n",
    "# 검증 데이터 정규화\n",
    "n_samples_val, ts_val, n_features_val = val_data.dataset[val_data.indices].shape\n",
    "val_data_reshaped = val_data.dataset[val_data.indices].reshape(-1, n_features_val)\n",
    "val_data_scaled_reshaped = scaler.transform(val_data_reshaped)\n",
    "val_sequences = val_data_scaled_reshaped.reshape(n_samples_val, ts_val, n_features_val)\n",
    "\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
    "\n",
    "train_dataset = PoseDataset(train_sequences)\n",
    "val_dataset = PoseDataset(val_sequences)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e76c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. LSTM 오토인코더 모델 정의 ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, latent_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=latent_dim, num_layers=1, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return hidden[-1]\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim_latent, n_output_features, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm = nn.LSTM(input_size=input_dim_latent, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, n_output_features)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "        outputs, _ = self.lstm(x)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions\n",
    "\n",
    "class LstmAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, latent_dim, hidden_dec_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(seq_len, n_features, latent_dim)\n",
    "        self.decoder = Decoder(seq_len, latent_dim, n_features, hidden_dec_dim)\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "519f0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 모델 구조 ---\n",
      "LstmAutoencoder(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(10, 8, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (lstm): LSTM(8, 64, batch_first=True)\n",
      "    (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "모델 파라미터 개수: 20234\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 모델, 손실함수, 옵티마이저 정의 ---\n",
    "model = LstmAutoencoder(TIME_STEPS, N_DERIVED_FEATURES, LATENT_DIM, HIDDEN_DEC_DIM).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\n--- 모델 구조 ---\")\n",
    "print(model)\n",
    "print(f\"모델 파라미터 개수: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "992eefa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 학습 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :   2%|▏         | 1/50 [00:05<04:29,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.321378, Val Loss: 0.188058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :   4%|▍         | 2/50 [00:10<04:23,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Train Loss: 0.153779, Val Loss: 0.128670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :   6%|▌         | 3/50 [00:16<04:18,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Train Loss: 0.117821, Val Loss: 0.107993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :   8%|▊         | 4/50 [00:21<04:12,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Train Loss: 0.104016, Val Loss: 0.098111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  10%|█         | 5/50 [00:27<04:07,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Train Loss: 0.095969, Val Loss: 0.091020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  12%|█▏        | 6/50 [00:32<04:01,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Train Loss: 0.090183, Val Loss: 0.086649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  14%|█▍        | 7/50 [00:38<03:56,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Train Loss: 0.085942, Val Loss: 0.084972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  16%|█▌        | 8/50 [00:43<03:50,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Train Loss: 0.082821, Val Loss: 0.082251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  18%|█▊        | 9/50 [00:49<03:45,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Train Loss: 0.080334, Val Loss: 0.079367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  20%|██        | 10/50 [00:54<03:39,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Train Loss: 0.078269, Val Loss: 0.077067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  22%|██▏       | 11/50 [01:00<03:33,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Train Loss: 0.076436, Val Loss: 0.075266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  24%|██▍       | 12/50 [01:05<03:28,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Train Loss: 0.074918, Val Loss: 0.074308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  26%|██▌       | 13/50 [01:11<03:22,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Train Loss: 0.073580, Val Loss: 0.071960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  28%|██▊       | 14/50 [01:16<03:17,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Train Loss: 0.072475, Val Loss: 0.071219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  30%|███       | 15/50 [01:22<03:11,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Train Loss: 0.071554, Val Loss: 0.071694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  32%|███▏      | 16/50 [01:27<03:06,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Train Loss: 0.070553, Val Loss: 0.070131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  34%|███▍      | 17/50 [01:33<03:00,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Train Loss: 0.069760, Val Loss: 0.068905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  36%|███▌      | 18/50 [01:38<02:55,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Train Loss: 0.068922, Val Loss: 0.068520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  38%|███▊      | 19/50 [01:44<02:49,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Train Loss: 0.068259, Val Loss: 0.068614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  40%|████      | 20/50 [01:49<02:44,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Train Loss: 0.067496, Val Loss: 0.067778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  42%|████▏     | 21/50 [01:55<02:38,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Train Loss: 0.067058, Val Loss: 0.066330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  44%|████▍     | 22/50 [02:00<02:33,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/50], Train Loss: 0.066320, Val Loss: 0.065127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  46%|████▌     | 23/50 [02:06<02:27,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/50], Train Loss: 0.065990, Val Loss: 0.066869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  48%|████▊     | 24/50 [02:11<02:22,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/50], Train Loss: 0.065455, Val Loss: 0.064478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  50%|█████     | 25/50 [02:17<02:16,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/50], Train Loss: 0.064945, Val Loss: 0.065152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  52%|█████▏    | 26/50 [02:22<02:11,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/50], Train Loss: 0.064642, Val Loss: 0.064053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  54%|█████▍    | 27/50 [02:27<02:05,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/50], Train Loss: 0.064008, Val Loss: 0.063060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  56%|█████▌    | 28/50 [02:33<02:00,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/50], Train Loss: 0.063704, Val Loss: 0.063188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  58%|█████▊    | 29/50 [02:38<01:54,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/50], Train Loss: 0.063286, Val Loss: 0.063281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  60%|██████    | 30/50 [02:44<01:49,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/50], Train Loss: 0.062933, Val Loss: 0.062564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  62%|██████▏   | 31/50 [02:49<01:43,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/50], Train Loss: 0.062550, Val Loss: 0.063326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  64%|██████▍   | 32/50 [02:55<01:38,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/50], Train Loss: 0.062270, Val Loss: 0.062329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  66%|██████▌   | 33/50 [03:00<01:32,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/50], Train Loss: 0.061956, Val Loss: 0.061596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  68%|██████▊   | 34/50 [03:06<01:27,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/50], Train Loss: 0.061649, Val Loss: 0.061477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  70%|███████   | 35/50 [03:11<01:22,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/50], Train Loss: 0.061484, Val Loss: 0.061589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  72%|███████▏  | 36/50 [03:17<01:16,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/50], Train Loss: 0.061102, Val Loss: 0.061731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  74%|███████▍  | 37/50 [03:22<01:11,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/50], Train Loss: 0.060975, Val Loss: 0.061017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  76%|███████▌  | 38/50 [03:28<01:05,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/50], Train Loss: 0.060684, Val Loss: 0.060747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  78%|███████▊  | 39/50 [03:33<01:00,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/50], Train Loss: 0.060403, Val Loss: 0.060538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  80%|████████  | 40/50 [03:39<00:54,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/50], Train Loss: 0.060130, Val Loss: 0.059444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  82%|████████▏ | 41/50 [03:44<00:49,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/50], Train Loss: 0.059918, Val Loss: 0.060358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  84%|████████▍ | 42/50 [03:50<00:43,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/50], Train Loss: 0.059772, Val Loss: 0.059652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  86%|████████▌ | 43/50 [03:55<00:38,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/50], Train Loss: 0.060459, Val Loss: 0.059585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  88%|████████▊ | 44/50 [04:01<00:32,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/50], Train Loss: 0.059261, Val Loss: 0.059090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  90%|█████████ | 45/50 [04:06<00:27,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/50], Train Loss: 0.059209, Val Loss: 0.059608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  92%|█████████▏| 46/50 [04:12<00:21,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/50], Train Loss: 0.058944, Val Loss: 0.058793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  94%|█████████▍| 47/50 [04:17<00:16,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/50], Train Loss: 0.059182, Val Loss: 0.059236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  96%|█████████▌| 48/50 [04:23<00:10,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/50], Train Loss: 0.058553, Val Loss: 0.058847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs :  98%|█████████▊| 49/50 [04:28<00:05,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/50], Train Loss: 0.058433, Val Loss: 0.058952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/50], Train Loss: 0.058311, Val Loss: 0.057690\n",
      "\n",
      "학습 완료!\n",
      "최저 검증 손실: 0.057690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# --- 5. 학습 루프 ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in tqdm(range(num_epochs), leave=False, desc=\"Epochs : \"):\n",
    "        model.train() # 학습 모드\n",
    "        running_train_loss = 0.0\n",
    "        for i, sequences in enumerate(train_loader):\n",
    "            sequences = sequences.to(device) # (batch, time_steps, n_derived_features)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(sequences)\n",
    "            loss = criterion(reconstructed, sequences) # 입력 자신을 타겟으로\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item() * sequences.size(0) # 배치 손실 누적\n",
    "        \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # 검증\n",
    "        model.eval() # 평가 모드\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for sequences in val_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                reconstructed = model(sequences)\n",
    "                loss = criterion(reconstructed, sequences)\n",
    "                running_val_loss += loss.item() * sequences.size(0)\n",
    "        \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.6f}, Val Loss: {epoch_val_loss:.6f}\")\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), 'best_lstm_autoencoder_model.pth')\n",
    "            # print(f\"Validation loss decreased. Saving model to best_lstm_autoencoder_model.pth\")\n",
    "\n",
    "    print(\"\\n학습 완료!\")\n",
    "    print(f\"최저 검증 손실: {best_val_loss:.6f}\")\n",
    "\n",
    "if raw_windows.shape[0] > BATCH_SIZE * 2 : # 충분한 데이터가 있을 때만 학습\n",
    "    print(\"\\n--- 학습 시작 ---\")\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, device)\n",
    "else:\n",
    "    print(\"데이터가 부족하여 학습을 진행하지 않습니다.\")\n",
    "    if raw_windows.shape[0] > 0 :\n",
    "         print(\"raw_windows 형태:\", raw_windows.shape)\n",
    "         print(\"정규화된 train_sequences 형태:\", train_sequences.shape if 'train_sequences' in locals() else \"N/A\")\n",
    "         print(\"정규화된 val_sequences 형태:\", val_sequences.shape if 'val_sequences' in locals() else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "932c9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "저장된 모델 로드 성공!\n"
     ]
    }
   ],
   "source": [
    "# 이전에 정의한 모델 클래스들 (Encoder, Decoder, LstmAutoencoder)과\n",
    "# 하이퍼파라미터 (TIME_STEPS, N_DERIVED_FEATURES 등)가 동일하게 정의되어 있어야 합니다.\n",
    "\n",
    "# 모델 인스턴스 다시 생성\n",
    "loaded_model = LstmAutoencoder(TIME_STEPS, N_DERIVED_FEATURES, LATENT_DIM, HIDDEN_DEC_DIM).to(device)\n",
    "\n",
    "# 저장된 가중치 로드\n",
    "try:\n",
    "    loaded_model.load_state_dict(torch.load('best_lstm_autoencoder_model.pth', map_location=device))\n",
    "    loaded_model.eval() # 평가 모드로 설정 (매우 중요!)\n",
    "    print(\"\\n저장된 모델 로드 성공!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"오류: 저장된 모델 파일(best_lstm_autoencoder_model.pth)을 찾을 수 없습니다.\")\n",
    "    print(\"먼저 모델 학습을 완료해주세요.\")\n",
    "    # 이 경우 아래 코드는 실행할 수 없으므로 종료 또는 예외 처리\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"모델 로드 중 오류 발생: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533f9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결정된 재구성 오차 임계값: 0.198860\n"
     ]
    }
   ],
   "source": [
    "val_reconstruction_errors = []\n",
    "loaded_model.eval() # 평가 모드\n",
    "with torch.no_grad(): # 그래디언트 계산 비활성화\n",
    "    for sequences in val_loader: # val_loader는 정규화된 데이터를 로드해야 함\n",
    "        sequences = sequences.to(device)\n",
    "        reconstructed = loaded_model(sequences)\n",
    "        # 각 시퀀스 샘플별 MSE 계산\n",
    "        # (batch, time_steps, features) -> (batch, time_steps * features)\n",
    "        # 이후 각 샘플별로 평균 제곱 오차 계산\n",
    "        mse = torch.mean((sequences - reconstructed)**2, dim=[1, 2]) # 각 윈도우(샘플)별 MSE\n",
    "        val_reconstruction_errors.extend(mse.cpu().numpy())\n",
    "\n",
    "val_reconstruction_errors = np.array(val_reconstruction_errors)\n",
    "\n",
    "if len(val_reconstruction_errors) > 0:\n",
    "    # 재구성 오차 분포 확인 (예: 히스토그램, 기술 통계)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.hist(val_reconstruction_errors, bins=50, alpha=0.7, label='Validation Reconstruction Errors')\n",
    "    plt.xlabel(\"Reconstruction MSE\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Reconstruction Errors on Validation Set\")\n",
    "    \n",
    "    # 임계값 결정 (예: 95 백분위수 또는 특정 값)\n",
    "    # 이 값은 실험을 통해 조절해야 합니다.\n",
    "    THRESHOLD = np.percentile(val_reconstruction_errors, 98) # 예: 상위 5%를 이상치로 간주\n",
    "    # 또는 THRESHOLD = np.mean(val_reconstruction_errors) + 2 * np.std(val_reconstruction_errors)\n",
    "    \n",
    "    plt.axvline(THRESHOLD, color='r', linestyle='dashed', linewidth=2, label=f'Threshold ({THRESHOLD:.6f})')\n",
    "    plt.legend()\n",
    "    plt.show() # Colab이나 Jupyter Notebook 환경에서는 plt.show()가 그래프를 표시\n",
    "    print(f\"\\n결정된 재구성 오차 임계값: {THRESHOLD:.6f}\")\n",
    "else:\n",
    "    print(\"검증 데이터에 대한 재구성 오차를 계산할 수 없습니다. 임계값 설정이 불가능합니다.\")\n",
    "    THRESHOLD = float('inf') # 임시 임계값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65ae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 새로운 비디오(/data/hamboong/tmp/cap/data/mediapipe_res/good_test/clean_squat_squat_000013_res.npy)에 대한 이상치 탐지 ---\n",
      "총 271개의 윈도우 분석 완료. 이상치 감지되지 않음.\n"
     ]
    }
   ],
   "source": [
    "def detect_anomalies_in_video(video_path, model, scaler, threshold,\n",
    "                               time_steps, stride, feature_extraction_fn, device):\n",
    "    model.eval()\n",
    "    all_windows_in_video = []\n",
    "    anomalous_window_indices = [] # 이상치로 판단된 윈도우의 인덱스 저장\n",
    "\n",
    "    try:\n",
    "        landmarks_video = np.load(video_path)\n",
    "        if landmarks_video.ndim != 3 or landmarks_video.shape[1] != 12 or landmarks_video.shape[2] != 3:\n",
    "            print(f\"오류: {video_path} 파일 형태가 올바르지 않습니다.\")\n",
    "            return [], [] # 빈 리스트 반환\n",
    "\n",
    "        derived_features_video = []\n",
    "        for frame_idx in range(landmarks_video.shape[0]):\n",
    "            landmarks_frame = landmarks_video[frame_idx, :, :]\n",
    "            derived_features = feature_extraction_fn(landmarks_frame)\n",
    "            derived_features_video.append(derived_features)\n",
    "        derived_features_video = np.array(derived_features_video)\n",
    "\n",
    "        # 슬라이딩 윈도우 생성\n",
    "        for i in range(0, derived_features_video.shape[0] - time_steps + 1, stride):\n",
    "            sequence = derived_features_video[i : i + time_steps]\n",
    "            all_windows_in_video.append(sequence)\n",
    "        \n",
    "        if not all_windows_in_video:\n",
    "            print(f\"{video_path}에서 윈도우를 생성할 수 없습니다.\")\n",
    "            return [], []\n",
    "\n",
    "        all_windows_np = np.array(all_windows_in_video, dtype=np.float32)\n",
    "\n",
    "        # NaN 포함 윈도우 처리 (여기서는 그냥 해당 윈도우는 정상으로 간주하거나, NaN으로 표시)\n",
    "        # 또는 학습 데이터처럼 제거할 수도 있지만, 예측 시에는 모든 윈도우에 대한 판단이 필요할 수 있음\n",
    "        window_errors = []\n",
    "        original_indices = [] # NaN이 아닌 윈도우의 원본 인덱스\n",
    "\n",
    "        for idx, window in enumerate(all_windows_np):\n",
    "            if np.isnan(window).any():\n",
    "                window_errors.append(0) # NaN 포함 윈도우는 오차 0 (정상)으로 간주하거나 다른 처리\n",
    "                # 또는 window_errors.append(np.nan) 등으로 표시\n",
    "                continue\n",
    "            \n",
    "            original_indices.append(idx) # NaN이 아닌 윈도우의 인덱스 저장\n",
    "            # 정규화 (학습 시 사용한 scaler와 동일하게)\n",
    "            # scaler는 2D 입력을 기대하므로 reshape\n",
    "            window_reshaped = window.reshape(-1, window.shape[-1])\n",
    "            window_scaled_reshaped = scaler.transform(window_reshaped)\n",
    "            window_scaled = window_scaled_reshaped.reshape(window.shape)\n",
    "            \n",
    "            window_tensor = torch.tensor(window_scaled, dtype=torch.float32).unsqueeze(0).to(device) # 배치 차원 추가\n",
    "\n",
    "            with torch.no_grad():\n",
    "                reconstructed = model(window_tensor)\n",
    "                mse = torch.mean((window_tensor - reconstructed)**2).item() # 단일 윈도우에 대한 MSE\n",
    "            window_errors.append(mse)\n",
    "\n",
    "        window_errors_np = np.array(window_errors)\n",
    "        \n",
    "        # NaN이 아닌 윈도우들에 대해서만 이상치 판단\n",
    "        # (만약 NaN 윈도우를 다른 값으로 채웠다면, 모든 window_errors_np에 대해 판단)\n",
    "        for i, original_idx in enumerate(original_indices):\n",
    "            if window_errors_np[original_idx] > threshold : # NaN 윈도우는 이 조건문에 들어오지 않음\n",
    "                 anomalous_window_indices.append(original_idx)\n",
    "        \n",
    "        return window_errors_np, anomalous_window_indices\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: 파일을 찾을 수 없습니다 -> {video_path}\")\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        print(f\"'{video_path}' 처리 중 오류: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# --- 이상치 탐지 실행 예시 ---\n",
    "# 새로운 비디오 파일 경로 (테스트용)\n",
    "# 실제로는 사용자가 업로드하거나 실시간 스트리밍으로부터 .npy 형태로 변환된 데이터를 가정\n",
    "new_video_npy_path = './../data/mediapipe_res/good_test/clean_squat_squat_000013_res.npy' # 예시 경로\n",
    "\n",
    "\n",
    "if 'loaded_model' in globals() and 'scaler' in globals() and THRESHOLD != float('inf'):\n",
    "    print(f\"\\n--- 새로운 비디오({new_video_npy_path})에 대한 이상치 탐지 ---\")\n",
    "    # 더미 파일 생성 (테스트용)\n",
    "    if not os.path.exists(new_video_npy_path):\n",
    "        print(f\"경고: 테스트 파일({new_video_npy_path})이 없습니다. 더미 데이터를 생성합니다.\")\n",
    "        dummy_new_data = np.random.rand(300, 12, 3).astype(np.float32) # (프레임, 12랜드마크, 3좌표)\n",
    "        # 디렉토리 확인 및 생성\n",
    "        os.makedirs(os.path.dirname(new_video_npy_path), exist_ok=True)\n",
    "        np.save(new_video_npy_path, dummy_new_data)\n",
    "\n",
    "    errors, anomalies = detect_anomalies_in_video(\n",
    "        new_video_npy_path,\n",
    "        loaded_model,\n",
    "        scaler, # 학습 시 사용한 scaler\n",
    "        THRESHOLD,\n",
    "        TIME_STEPS,\n",
    "        WINDOW_STRIDE,\n",
    "        extract_derived_features, # 사용자 정의 함수\n",
    "        device\n",
    "    )\n",
    "\n",
    "    if anomalies:\n",
    "        print(f\"총 {len(errors)}개의 윈도우 중 {len(anomalies)}개의 이상치 윈도우 감지:\")\n",
    "        print(\"이상치 윈도우 인덱스:\", anomalies)\n",
    "        # print(\"각 윈도우별 재구성 오차:\", errors) # 모든 윈도우 오차 확인\n",
    "    elif errors.size > 0 : # errors가 비어있지 않고 anomalies가 비어있다면\n",
    "        print(f\"총 {len(errors)}개의 윈도우 분석 완료. 이상치 감지되지 않음.\")\n",
    "        # print(\"각 윈도우별 재구성 오차:\", errors)\n",
    "    else:\n",
    "        print(f\"{new_video_npy_path} 분석 중 문제가 발생했거나 윈도우가 생성되지 않았습니다.\")\n",
    "\n",
    "else:\n",
    "    if THRESHOLD == float('inf'):\n",
    "        print(\"\\n임계값이 제대로 설정되지 않아 이상치 탐지를 실행할 수 없습니다.\")\n",
    "    else:\n",
    "        print(\"\\n모델 또는 스케일러가 로드되지 않아 이상치 탐지를 실행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0ce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 'good' 폴더 평가 시작 (268개 파일) ---\n",
      "\n",
      "--- 'bad_innner_thigh' 폴더 평가 시작 (34개 파일) ---\n",
      "\n",
      "\n",
      "--- 최종 평가 결과 ---\n",
      "전체 정확도 (Overall Accuracy): 0.8841\n",
      "'나쁜 자세'에 대한 재현율 (Recall for 'bad' class): 0.8529\n",
      "\n",
      "--- Classification Report (Combined) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_pose       0.98      0.89      0.93       268\n",
      "    bad_pose       0.49      0.85      0.62        34\n",
      "\n",
      "    accuracy                           0.88       302\n",
      "   macro avg       0.74      0.87      0.78       302\n",
      "weighted avg       0.92      0.88      0.90       302\n",
      "\n",
      "\n",
      "--- Confusion Matrix (Combined) ---\n",
      "레이블: [0 (Good), 1 (Bad)]\n",
      "[[238  30]\n",
      " [  5  29]]\n",
      "\n",
      "전체 스크립트 실행 완료.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim # 평가 시에는 필요 없음\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split # 평가 시에는 필요 없음\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "# from sklearn.preprocessing import StandardScaler # scaler는 이미 학습/로드 되었다고 가정\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score # 추가\n",
    "# import matplotlib.pyplot as plt # 여기서는 직접 사용하지 않음\n",
    "\n",
    "def evaluate_model_performance(data_dir, true_label, model, scaler, threshold,\n",
    "                               time_steps, stride, feature_extraction_fn, device):\n",
    "    \"\"\"지정된 디렉토리의 모든 .npy 파일에 대해 이상치 탐지를 수행하고 예측 결과를 반환합니다.\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    npy_files = glob.glob(os.path.join(data_dir, \"*.npy\"))\n",
    "    if not npy_files:\n",
    "        print(f\"경고: {data_dir} 에서 평가할 .npy 파일을 찾을 수 없습니다.\")\n",
    "        return y_true, y_pred\n",
    "\n",
    "    print(f\"\\n--- '{os.path.basename(data_dir)}' 폴더 평가 시작 ({len(npy_files)}개 파일) ---\")\n",
    "    for npy_file in npy_files:\n",
    "        # print(f\"파일 평가 중: {os.path.basename(npy_file)}\")\n",
    "        \n",
    "        # detect_anomalies_in_video 함수는 여기서 정의되어야 합니다.\n",
    "        # 이전 답변에 있는 함수를 사용한다고 가정합니다.\n",
    "        # 여기서는 함수의 핵심 로직을 간략히 다시 구현하거나, 해당 함수를 호출합니다.\n",
    "        \n",
    "        # ---- detect_anomalies_in_video 내부 로직 간소화 버전 (실제로는 함수 호출) ----\n",
    "        processed_correctly = True\n",
    "        anomalies_found_in_video = False\n",
    "        try:\n",
    "            landmarks_video = np.load(npy_file)\n",
    "            if landmarks_video.ndim != 3 or landmarks_video.shape[1] != 12 or landmarks_video.shape[2] != 3:\n",
    "                print(f\"  경고: {os.path.basename(npy_file)} 파일 형태가 올바르지 않음. 건너뜀.\")\n",
    "                processed_correctly = False\n",
    "            \n",
    "            if processed_correctly:\n",
    "                derived_features_video = np.array([feature_extraction_fn(landmarks_video[i]) for i in range(landmarks_video.shape[0])])\n",
    "                \n",
    "                current_video_windows = []\n",
    "                for i in range(0, derived_features_video.shape[0] - time_steps + 1, stride):\n",
    "                    sequence = derived_features_video[i : i + time_steps]\n",
    "                    current_video_windows.append(sequence)\n",
    "                \n",
    "                if not current_video_windows:\n",
    "                    processed_correctly = False\n",
    "                \n",
    "                if processed_correctly:\n",
    "                    current_video_windows_np = np.array(current_video_windows, dtype=np.float32)\n",
    "                    nan_mask = ~np.isnan(current_video_windows_np).any(axis=(1,2))\n",
    "                    valid_windows = current_video_windows_np[nan_mask]\n",
    "\n",
    "                    if valid_windows.shape[0] == 0:\n",
    "                        processed_correctly = False # 유효한 윈도우 없음\n",
    "                    \n",
    "                    if processed_correctly:\n",
    "                        # 정규화\n",
    "                        valid_windows_reshaped = valid_windows.reshape(-1, valid_windows.shape[-1])\n",
    "                        valid_windows_scaled_reshaped = scaler.transform(valid_windows_reshaped)\n",
    "                        valid_windows_scaled = valid_windows_scaled_reshaped.reshape(valid_windows.shape)\n",
    "                        \n",
    "                        windows_tensor = torch.tensor(valid_windows_scaled, dtype=torch.float32).to(device)\n",
    "                        \n",
    "                        model.eval()\n",
    "                        with torch.no_grad():\n",
    "                            reconstructed = model(windows_tensor)\n",
    "                            mse_per_window = torch.mean((windows_tensor - reconstructed)**2, dim=[1,2]).cpu().numpy()\n",
    "                        \n",
    "                        if np.any(mse_per_window > threshold):\n",
    "                            anomalies_found_in_video = True\n",
    "        except Exception as e:\n",
    "            print(f\"  오류: {os.path.basename(npy_file)} 처리 중 문제 발생 - {e}\")\n",
    "            processed_correctly = False\n",
    "        # ---- detect_anomalies_in_video 내부 로직 간소화 버전 끝 ----\n",
    "\n",
    "        if processed_correctly:\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(1 if anomalies_found_in_video else 0)\n",
    "            # print(f\"  결과: {'이상치 감지됨' if anomalies_found_in_video else '이상치 없음'}\")\n",
    "        else:\n",
    "            # 파일 처리 실패 시 해당 샘플은 평가에서 제외하거나, 별도 처리\n",
    "            print(f\"  {os.path.basename(npy_file)} 파일 처리에 실패하여 평가에서 제외될 수 있습니다.\")\n",
    "            \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "# --- 스크립트 실행 부분 ---\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    DATA_DIR_GOOD = \"./../data/mediapipe_res/good\" # 학습에 사용된 좋은 자세 데이터 경로\n",
    "    BAD_VIDEO_DIR = \"./../data/mediapipe_res/bad_innner_thigh\" # 테스트할 나쁜 자세 데이터 경로\n",
    "\n",
    "    # \"좋은 자세\" 데이터 평가 (실제 레이블: 0)\n",
    "    y_true_good, y_pred_good = evaluate_model_performance(\n",
    "        DATA_DIR_GOOD, 0, loaded_model, scaler, THRESHOLD,\n",
    "        TIME_STEPS, WINDOW_STRIDE, extract_derived_features, device\n",
    "    )\n",
    "\n",
    "    # \"나쁜 자세\" 데이터 평가 (실제 레이블: 1)\n",
    "    y_true_bad, y_pred_bad = evaluate_model_performance(\n",
    "        BAD_VIDEO_DIR, 1, loaded_model, scaler, THRESHOLD,\n",
    "        TIME_STEPS, WINDOW_STRIDE, extract_derived_features, device\n",
    "    )\n",
    "\n",
    "    # 전체 결과 취합\n",
    "    y_true_combined = np.array(y_true_good + y_true_bad)\n",
    "    y_pred_combined = np.array(y_pred_good + y_pred_bad)\n",
    "\n",
    "    if len(y_true_combined) > 0 and len(y_true_combined) == len(y_pred_combined):\n",
    "        accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
    "        # 'bad' 클래스(레이블 1)에 대한 재현율\n",
    "        recall_bad_class = recall_score(y_true_combined, y_pred_combined, pos_label=1, zero_division=0)\n",
    "        # 'good' 클래스(레이블 0)에 대한 재현율 (Specificity와 유사)\n",
    "        # recall_good_class = recall_score(y_true_combined, y_pred_combined, pos_label=0, zero_division=0)\n",
    "\n",
    "\n",
    "        print(\"\\n\\n--- 최종 평가 결과 ---\")\n",
    "        print(f\"전체 정확도 (Overall Accuracy): {accuracy:.4f}\")\n",
    "        print(f\"'나쁜 자세'에 대한 재현율 (Recall for 'bad' class): {recall_bad_class:.4f}\")\n",
    "        # print(f\"'좋은 자세'에 대한 재현율 (Recall for 'good' class / Specificity): {recall_good_class:.4f}\")\n",
    "        \n",
    "        print(\"\\n--- Classification Report (Combined) ---\")\n",
    "        # target_names는 레이블 순서에 맞게 [0, 1] -> ['good_pose', 'bad_pose']\n",
    "        print(classification_report(y_true_combined, y_pred_combined, target_names=['good_pose', 'bad_pose'], zero_division=0))\n",
    "\n",
    "        print(\"\\n--- Confusion Matrix (Combined) ---\")\n",
    "        # 레이블: [0 (Good), 1 (Bad)]\n",
    "        #       Predicted Good | Predicted Bad\n",
    "        # True Good TN         | FP\n",
    "        # True Bad  FN         | TP\n",
    "        cm = confusion_matrix(y_true_combined, y_pred_combined, labels=[0,1])\n",
    "        print(\"레이블: [0 (Good), 1 (Bad)]\")\n",
    "        print(cm)\n",
    "\n",
    "    else:\n",
    "        print(\"\\n평가할 데이터가 충분하지 않거나, 결과 취합에 문제가 있습니다.\")\n",
    "\n",
    "    print(\"\\n전체 스크립트 실행 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5dd16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- '/data/hamboong/tmp/cap/data/mediapipe_res/bad_innner_thigh' 폴더 내 파일들에 대한 이상치 평가 ---\n",
      "파일 평가 중: 0918_squat_000029_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 170개)\n",
      "파일 평가 중: 0918_squat_000030_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 119개)\n",
      "파일 평가 중: 0918_squat_000032_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 136개)\n",
      "파일 평가 중: 0918_squat_000033_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 219개)\n",
      "파일 평가 중: 0918_squat_000036_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 111개)\n",
      "파일 평가 중: 0918_squat_000066_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 177개)\n",
      "파일 평가 중: 0918_squat_000069_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 146개)\n",
      "파일 평가 중: 0918_squat_000070_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 206개)\n",
      "파일 평가 중: 0918_squat_000073_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 168개)\n",
      "파일 평가 중: 0922_squat_000040_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 137개)\n",
      "파일 평가 중: 0922_squat_000041_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 42개)\n",
      "파일 평가 중: 0922_squat_000043_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 109개)\n",
      "파일 평가 중: 0922_squat_000044_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 232개)\n",
      "파일 평가 중: 0922_squat_000047_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 78개)\n",
      "파일 평가 중: 0922_squat_000081_res.npy\n",
      "  결과: 이상치 감지됨 (총 271개 윈도우 중 168개)\n",
      "파일 평가 중: 0922_squat_000082_res.npy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 이 폴더의 모든 영상은 \"나쁜 자세\" (True Label = 1)라고 가정\u001b[39;00m\n\u001b[1;32m     30\u001b[0m y_true_bad_videos\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m errors, anomalies \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_anomalies_in_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnpy_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTHRESHOLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTIME_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mWINDOW_STRIDE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_derived_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(anomalies) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# 하나 이상의 윈도우라도 이상치로 감지되면 해당 영상은 \"bad\"로 예측\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     y_pred_bad_videos\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[57], line 16\u001b[0m, in \u001b[0;36mdetect_anomalies_in_video\u001b[0;34m(video_path, model, scaler, threshold, time_steps, stride, feature_extraction_fn, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(landmarks_video\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     15\u001b[0m     landmarks_frame \u001b[38;5;241m=\u001b[39m landmarks_video[frame_idx, :, :]\n\u001b[0;32m---> 16\u001b[0m     derived_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extraction_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlandmarks_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     derived_features_video\u001b[38;5;241m.\u001b[39mappend(derived_features)\n\u001b[1;32m     18\u001b[0m derived_features_video \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(derived_features_video)\n",
      "Cell \u001b[0;32mIn[72], line 26\u001b[0m, in \u001b[0;36mextract_derived_features\u001b[0;34m(landmarks_frame)\u001b[0m\n\u001b[1;32m     24\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(calculate_angle(r_hip, r_knee, r_ankle))\n\u001b[1;32m     25\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(calculate_angle(l_shoulder, l_hip, l_knee))\n\u001b[0;32m---> 26\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcalculate_angle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr_shoulder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_hip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_knee\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     27\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(calculate_angle(l_knee, l_ankle, l_foot_index))\n\u001b[1;32m     28\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(calculate_angle(r_knee, r_ankle, r_foot_index))\n",
      "Cell \u001b[0;32mIn[72], line 11\u001b[0m, in \u001b[0;36mcalculate_angle\u001b[0;34m(a, b, c)\u001b[0m\n\u001b[1;32m      9\u001b[0m cosine_angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(ba, bc) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(ba) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(bc) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m) \u001b[38;5;66;03m# 분모 0 방지\u001b[39;00m\n\u001b[1;32m     10\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marccos(np\u001b[38;5;241m.\u001b[39mclip(cosine_angle, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegrees\u001b[49m\u001b[43m(\u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score # 추가\n",
    "import matplotlib.pyplot as plt # THRESHOLD 시각화용\n",
    "\n",
    "\n",
    "# --- 1. 하이퍼파라미터 및 설정 (이전 코드와 동일) ---\n",
    "DATA_DIR_GOOD = \"./../data/mediapipe_res/good\" # 학습에 사용된 좋은 자세 데이터 경로\n",
    "BAD_VIDEO_DIR = \"./../data/mediapipe_res/bad_innner_thigh\" # 테스트할 나쁜 자세 데이터 경로\n",
    "\n",
    "# --- 5. \"bad_innner_thigh\" 폴더 내 파일들에 대한 평가 ---\n",
    "print(f\"\\n--- '{BAD_VIDEO_DIR}' 폴더 내 파일들에 대한 이상치 평가 ---\")\n",
    "bad_npy_files = glob.glob(os.path.join(BAD_VIDEO_DIR, \"*.npy\"))\n",
    "\n",
    "if not bad_npy_files:\n",
    "    print(f\"경고: {BAD_VIDEO_DIR} 에서 평가할 .npy 파일을 찾을 수 없습니다.\")\n",
    "else:\n",
    "    y_true_bad_videos = [] # 실제로는 모두 'bad' (1)\n",
    "    y_pred_bad_videos = [] # 모델 예측: 'bad' (1) or 'not_bad' (0)\n",
    "\n",
    "    for npy_file in bad_npy_files:\n",
    "        print(f\"파일 평가 중: {os.path.basename(npy_file)}\")\n",
    "        # 이 폴더의 모든 영상은 \"나쁜 자세\" (True Label = 1)라고 가정\n",
    "        y_true_bad_videos.append(1)\n",
    "\n",
    "        errors, anomalies = detect_anomalies_in_video(\n",
    "            npy_file,\n",
    "            loaded_model,\n",
    "            scaler,\n",
    "            THRESHOLD,\n",
    "            TIME_STEPS,\n",
    "            WINDOW_STRIDE,\n",
    "            extract_derived_features,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        if len(anomalies) > 0: # 하나 이상의 윈도우라도 이상치로 감지되면 해당 영상은 \"bad\"로 예측\n",
    "            y_pred_bad_videos.append(1)\n",
    "            print(f\"  결과: 이상치 감지됨 (총 {len(errors)}개 윈도우 중 {len(anomalies)}개)\")\n",
    "        elif errors.size > 0 : # 오류 없이 분석 완료되었으나 이상치 없음\n",
    "            y_pred_bad_videos.append(0)\n",
    "            print(f\"  결과: 이상치 감지되지 않음 (총 {len(errors)}개 윈도우 분석)\")\n",
    "        else: # 파일 처리 중 문제 발생 (예: 파일 못읽음, 윈도우 생성 안됨)\n",
    "            # 이런 경우는 평가에서 제외하거나, 특정 값으로 처리해야 함\n",
    "            # 여기서는 예측 실패로 간주하고, 실제 값과 다른 값으로 예측했다고 가정 (예: 0)\n",
    "            # 또는 y_true_bad_videos에서도 해당 샘플을 제거해야 함\n",
    "            print(f\"  결과: 파일 처리 오류 또는 유효 윈도우 없음. 평가에서 제외될 수 있음.\")\n",
    "            # y_true_bad_videos.pop() # 만약 해당 샘플을 평가에서 제외하려면\n",
    "\n",
    "    if y_true_bad_videos and y_pred_bad_videos and len(y_true_bad_videos) == len(y_pred_bad_videos):\n",
    "        print(\"\\n--- Classification Report (Bad Videos) ---\")\n",
    "        # 레이블 0: \"not_bad\" (모델이 나쁜 자세를 나쁘다고 판단 못한 경우)\n",
    "        # 레이블 1: \"bad\" (모델이 나쁜 자세를 나쁘다고 판단한 경우)\n",
    "        target_names = ['predicted_not_bad', 'predicted_bad']\n",
    "        print(classification_report(y_true_bad_videos, y_pred_bad_videos, target_names=target_names, zero_division=0))\n",
    "        \n",
    "        print(\"\\n--- Confusion Matrix (Bad Videos) ---\")\n",
    "        # 행: 실제 클래스, 열: 예측 클래스\n",
    "        # 이 경우 실제는 모두 'bad'(1)이므로, Confusion Matrix의 첫번째 행은 모두 0이어야 함.\n",
    "        # [[TN, FP],\n",
    "        #  [FN, TP]]\n",
    "        # 여기서는 True Negative (TN)와 False Positive (FP)는 나올 수 없음 (y_true가 항상 1이므로)\n",
    "        # 결과적으로는 [[0, 0], [FN, TP]] 형태가 됨\n",
    "        cm = confusion_matrix(y_true_bad_videos, y_pred_bad_videos, labels=[0,1])\n",
    "        print(\"레이블: [0 (Not Bad), 1 (Bad)]\")\n",
    "        print(cm)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true_bad_videos, y_pred_bad_videos)\n",
    "        print(f\"\\n정확도 (Accuracy on these 'bad' videos): {accuracy:.4f}\")\n",
    "        if len(y_true_bad_videos) > 0:\n",
    "            recall_bad = cm[1,1] / (cm[1,0] + cm[1,1]) if (cm[1,0] + cm[1,1]) > 0 else 0\n",
    "            print(f\"나쁜 자세 재현율 (Recall for 'bad' class): {recall_bad:.4f} (얼마나 많은 실제 '나쁜 자세' 영상을 '나쁘다'고 잡아냈는가)\")\n",
    "\n",
    "    else:\n",
    "        print(\"평가할 데이터가 충분하지 않거나, 실제/예측 레이블 개수가 맞지 않습니다.\")\n",
    "\n",
    "print(\"\\n전체 스크립트 실행 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06555a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 'good_test' 폴더 평가 시작 (33개 파일) ---\n",
      "\n",
      "--- 'bad_innner_thigh' 폴더 평가 시작 (34개 파일) ---\n",
      "\n",
      "\n",
      "--- 최종 평가 결과 ---\n",
      "전체 정확도 (Overall Accuracy): 0.5224\n",
      "'나쁜 자세'에 대한 재현율 (Recall for 'bad' class): 0.7059\n",
      "\n",
      "--- Classification Report (Combined) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_pose       0.52      0.33      0.41        33\n",
      "    bad_pose       0.52      0.71      0.60        34\n",
      "\n",
      "    accuracy                           0.52        67\n",
      "   macro avg       0.52      0.52      0.50        67\n",
      "weighted avg       0.52      0.52      0.51        67\n",
      "\n",
      "\n",
      "--- Confusion Matrix (Combined) ---\n",
      "레이블: [0 (Good), 1 (Bad)]\n",
      "[[11 22]\n",
      " [10 24]]\n",
      "\n",
      "전체 스크립트 실행 완료.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim # 평가 시에는 필요 없음\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split # 평가 시에는 필요 없음\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "# from sklearn.preprocessing import StandardScaler # scaler는 이미 학습/로드 되었다고 가정\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score # 추가\n",
    "# import matplotlib.pyplot as plt # 여기서는 직접 사용하지 않음\n",
    "\n",
    "def evaluate_model_performance(data_dir, true_label, model, scaler, threshold,\n",
    "                               time_steps, stride, feature_extraction_fn, device):\n",
    "    \"\"\"지정된 디렉토리의 모든 .npy 파일에 대해 이상치 탐지를 수행하고 예측 결과를 반환합니다.\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    npy_files = glob.glob(os.path.join(data_dir, \"*.npy\"))\n",
    "    if not npy_files:\n",
    "        print(f\"경고: {data_dir} 에서 평가할 .npy 파일을 찾을 수 없습니다.\")\n",
    "        return y_true, y_pred\n",
    "\n",
    "    print(f\"\\n--- '{os.path.basename(data_dir)}' 폴더 평가 시작 ({len(npy_files)}개 파일) ---\")\n",
    "    for npy_file in npy_files:\n",
    "        # print(f\"파일 평가 중: {os.path.basename(npy_file)}\")\n",
    "        \n",
    "        # detect_anomalies_in_video 함수는 여기서 정의되어야 합니다.\n",
    "        # 이전 답변에 있는 함수를 사용한다고 가정합니다.\n",
    "        # 여기서는 함수의 핵심 로직을 간략히 다시 구현하거나, 해당 함수를 호출합니다.\n",
    "        \n",
    "        # ---- detect_anomalies_in_video 내부 로직 간소화 버전 (실제로는 함수 호출) ----\n",
    "        processed_correctly = True\n",
    "        anomalies_found_in_video = False\n",
    "        try:\n",
    "            landmarks_video = np.load(npy_file)\n",
    "            if landmarks_video.ndim != 3 or landmarks_video.shape[1] != 12 or landmarks_video.shape[2] != 3:\n",
    "                print(f\"  경고: {os.path.basename(npy_file)} 파일 형태가 올바르지 않음. 건너뜀.\")\n",
    "                processed_correctly = False\n",
    "            \n",
    "            if processed_correctly:\n",
    "                derived_features_video = np.array([feature_extraction_fn(landmarks_video[i]) for i in range(landmarks_video.shape[0])])\n",
    "                \n",
    "                current_video_windows = []\n",
    "                for i in range(0, derived_features_video.shape[0] - time_steps + 1, stride):\n",
    "                    sequence = derived_features_video[i : i + time_steps]\n",
    "                    current_video_windows.append(sequence)\n",
    "                \n",
    "                if not current_video_windows:\n",
    "                    processed_correctly = False\n",
    "                \n",
    "                if processed_correctly:\n",
    "                    current_video_windows_np = np.array(current_video_windows, dtype=np.float32)\n",
    "                    nan_mask = ~np.isnan(current_video_windows_np).any(axis=(1,2))\n",
    "                    valid_windows = current_video_windows_np[nan_mask]\n",
    "\n",
    "                    if valid_windows.shape[0] == 0:\n",
    "                        processed_correctly = False # 유효한 윈도우 없음\n",
    "                    \n",
    "                    if processed_correctly:\n",
    "                        # 정규화\n",
    "                        valid_windows_reshaped = valid_windows.reshape(-1, valid_windows.shape[-1])\n",
    "                        valid_windows_scaled_reshaped = scaler.transform(valid_windows_reshaped)\n",
    "                        valid_windows_scaled = valid_windows_scaled_reshaped.reshape(valid_windows.shape)\n",
    "                        \n",
    "                        windows_tensor = torch.tensor(valid_windows_scaled, dtype=torch.float32).to(device)\n",
    "                        \n",
    "                        model.eval()\n",
    "                        with torch.no_grad():\n",
    "                            reconstructed = model(windows_tensor)\n",
    "                            mse_per_window = torch.mean((windows_tensor - reconstructed)**2, dim=[1,2]).cpu().numpy()\n",
    "                        \n",
    "                        if np.any(mse_per_window > threshold):\n",
    "                            anomalies_found_in_video = True\n",
    "        except Exception as e:\n",
    "            print(f\"  오류: {os.path.basename(npy_file)} 처리 중 문제 발생 - {e}\")\n",
    "            processed_correctly = False\n",
    "        # ---- detect_anomalies_in_video 내부 로직 간소화 버전 끝 ----\n",
    "\n",
    "        if processed_correctly:\n",
    "            y_true.append(true_label)\n",
    "            y_pred.append(1 if anomalies_found_in_video else 0)\n",
    "            # print(f\"  결과: {'이상치 감지됨' if anomalies_found_in_video else '이상치 없음'}\")\n",
    "        else:\n",
    "            # 파일 처리 실패 시 해당 샘플은 평가에서 제외하거나, 별도 처리\n",
    "            print(f\"  {os.path.basename(npy_file)} 파일 처리에 실패하여 평가에서 제외될 수 있습니다.\")\n",
    "            \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "# --- 스크립트 실행 부분 ---\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    DATA_DIR_GOOD = \"./../data/mediapipe_res/good_test\" # 학습에 사용된 좋은 자세 데이터 경로\n",
    "    BAD_VIDEO_DIR = \"./../data/mediapipe_res/bad_innner_thigh\" # 테스트할 나쁜 자세 데이터 경로\n",
    "\n",
    "    # \"좋은 자세\" 데이터 평가 (실제 레이블: 0)\n",
    "    y_true_good, y_pred_good = evaluate_model_performance(\n",
    "        DATA_DIR_GOOD, 0, loaded_model, scaler, THRESHOLD,\n",
    "        TIME_STEPS, WINDOW_STRIDE, extract_derived_features, device\n",
    "    )\n",
    "\n",
    "    # \"나쁜 자세\" 데이터 평가 (실제 레이블: 1)\n",
    "    y_true_bad, y_pred_bad = evaluate_model_performance(\n",
    "        BAD_VIDEO_DIR, 1, loaded_model, scaler, THRESHOLD,\n",
    "        TIME_STEPS, WINDOW_STRIDE, extract_derived_features, device\n",
    "    )\n",
    "\n",
    "    # 전체 결과 취합\n",
    "    y_true_combined = np.array(y_true_good + y_true_bad)\n",
    "    y_pred_combined = np.array(y_pred_good + y_pred_bad)\n",
    "\n",
    "    if len(y_true_combined) > 0 and len(y_true_combined) == len(y_pred_combined):\n",
    "        accuracy = accuracy_score(y_true_combined, y_pred_combined)\n",
    "        # 'bad' 클래스(레이블 1)에 대한 재현율\n",
    "        recall_bad_class = recall_score(y_true_combined, y_pred_combined, pos_label=1, zero_division=0)\n",
    "        # 'good' 클래스(레이블 0)에 대한 재현율 (Specificity와 유사)\n",
    "        # recall_good_class = recall_score(y_true_combined, y_pred_combined, pos_label=0, zero_division=0)\n",
    "\n",
    "\n",
    "        print(\"\\n\\n--- 최종 평가 결과 ---\")\n",
    "        print(f\"전체 정확도 (Overall Accuracy): {accuracy:.4f}\")\n",
    "        print(f\"'나쁜 자세'에 대한 재현율 (Recall for 'bad' class): {recall_bad_class:.4f}\")\n",
    "        # print(f\"'좋은 자세'에 대한 재현율 (Recall for 'good' class / Specificity): {recall_good_class:.4f}\")\n",
    "        \n",
    "        print(\"\\n--- Classification Report (Combined) ---\")\n",
    "        # target_names는 레이블 순서에 맞게 [0, 1] -> ['good_pose', 'bad_pose']\n",
    "        print(classification_report(y_true_combined, y_pred_combined, target_names=['good_pose', 'bad_pose'], zero_division=0))\n",
    "\n",
    "        print(\"\\n--- Confusion Matrix (Combined) ---\")\n",
    "        # 레이블: [0 (Good), 1 (Bad)]\n",
    "        #       Predicted Good | Predicted Bad\n",
    "        # True Good TN         | FP\n",
    "        # True Bad  FN         | TP\n",
    "        cm = confusion_matrix(y_true_combined, y_pred_combined, labels=[0,1])\n",
    "        print(\"레이블: [0 (Good), 1 (Bad)]\")\n",
    "        print(cm)\n",
    "\n",
    "    else:\n",
    "        print(\"\\n평가할 데이터가 충분하지 않거나, 결과 취합에 문제가 있습니다.\")\n",
    "\n",
    "    print(\"\\n전체 스크립트 실행 완료.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
